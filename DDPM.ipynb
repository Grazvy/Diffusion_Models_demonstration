{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.cuda import amp\n",
    "import torchvision.transforms as TF\n",
    "from torchmetrics import MeanMetric\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from uNet import UNet\n",
    "from utils import get_default_device, make_grid, get, frames2vid, setup_log_directory\n",
    "from dataLoader import get_dataloader, inverse_transform"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-15T18:11:43.888706100Z",
     "start_time": "2024-06-15T18:11:38.927566Z"
    }
   },
   "id": "cb419348824b91b3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Configurations"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "482c246c5559b17c"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BaseConfig:\n",
    "    DEVICE = get_default_device()\n",
    "    DATASET = \"Flowers\" #  \"MNIST\", \"Cifar-10\", \"Cifar-100\", \"Flowers\"\n",
    "    \n",
    "    # For logging inferece images and saving checkpoints.\n",
    "    root_log_dir = os.path.join(\"Logs_Checkpoints\", \"Inference\")\n",
    "    root_checkpoint_dir = os.path.join(\"Logs_Checkpoints\", \"checkpoints\")\n",
    "\n",
    "    # Current log and checkpoint directory.\n",
    "    log_dir = \"version_0\"\n",
    "    checkpoint_dir = \"version_0\"\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    TIMESTEPS = 500 # Define number of diffusion timesteps\n",
    "    IMG_SHAPE = (1, 32, 32) if BaseConfig.DATASET == \"MNIST\" else (3, 32, 32) \n",
    "    NUM_EPOCHS = 200\n",
    "    BATCH_SIZE = 32\n",
    "    LR = 2e-4\n",
    "    NUM_WORKERS = 0 # does not work for windows\n",
    "    \n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    BASE_CH = 64  # 64, 128, 256, 256\n",
    "    BASE_CH_MULT = (1, 2, 4, 4) # 32, 16, 8, 8 \n",
    "    APPLY_ATTENTION = (False, True, True, False)\n",
    "    DROPOUT_RATE = 0.1\n",
    "    TIME_EMB_MULT = 4 # 128"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-15T18:13:14.972163100Z",
     "start_time": "2024-06-15T18:13:14.944631400Z"
    }
   },
   "id": "e8b70e1a3f9be547"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Visualize dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a647108fa506243e"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "#download data & provide it via loader-iterator\n",
    "loader = get_dataloader(\n",
    "    dataset_name=BaseConfig.DATASET,\n",
    "    batch_size=72,\n",
    "    device='cpu',\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-15T17:56:05.498083900Z",
     "start_time": "2024-06-15T17:56:05.367235500Z"
    }
   },
   "id": "ffec495a0df766"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6), facecolor='white')\n",
    "\n",
    "for b_image, _ in loader:\n",
    "    b_image = inverse_transform(b_image).cpu()\n",
    "    grid_img = make_grid(b_image / 255.0, nrow=12, padding=True, pad_value=1, normalize=True)\n",
    "    plt.imshow(grid_img.permute(1, 2, 0))\n",
    "    plt.axis(\"off\")\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3cf9193d16ba4d9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Diffusion Process"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f73271ce06330f5"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class SimpleDiffusion:\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_diffusion_timesteps=1000,\n",
    "        img_shape=(3, 64, 64),\n",
    "        device=\"cpu\",\n",
    "    ):\n",
    "        self.num_diffusion_timesteps = num_diffusion_timesteps\n",
    "        self.img_shape = img_shape\n",
    "        self.device = device\n",
    "\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        # BETAs & ALPHAs required at different places in the Algorithm.\n",
    "        self.beta  = self.get_betas()\n",
    "        self.alpha = 1 - self.beta\n",
    "        \n",
    "        self_sqrt_beta                       = torch.sqrt(self.beta)\n",
    "        self.alpha_cumulative                = torch.cumprod(self.alpha, dim=0)\n",
    "        self.sqrt_alpha_cumulative           = torch.sqrt(self.alpha_cumulative)\n",
    "        self.one_by_sqrt_alpha               = 1. / torch.sqrt(self.alpha)\n",
    "        self.sqrt_one_minus_alpha_cumulative = torch.sqrt(1 - self.alpha_cumulative)\n",
    "         \n",
    "    def get_betas(self):\n",
    "        \"\"\"linear schedule, proposed in original ddpm paper\"\"\"\n",
    "        scale = 1000 / self.num_diffusion_timesteps\n",
    "        beta_start = scale * 1e-4\n",
    "        beta_end = scale * 0.02\n",
    "        return torch.linspace(\n",
    "            beta_start,\n",
    "            beta_end,\n",
    "            self.num_diffusion_timesteps,\n",
    "            dtype=torch.float32,\n",
    "            device=self.device,\n",
    "        )\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-15T18:11:47.204759200Z",
     "start_time": "2024-06-15T18:11:47.184403100Z"
    }
   },
   "id": "2bc063382a4ae2f9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "\\Large x_t = \\sqrt{\\bar a_t}x_0 + \\sqrt{1-\\bar a_t}\\epsilon\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67dbee98119ee826"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def forward_diffusion(sd: SimpleDiffusion, x0: torch.Tensor, timesteps: torch.Tensor):\n",
    "    eps = torch.randn_like(x0)  # Noise\n",
    "    # image scaled + noise scaled\n",
    "    sample  = (get(sd.sqrt_alpha_cumulative, t=timesteps) * x0 + \n",
    "               get(sd.sqrt_one_minus_alpha_cumulative, t=timesteps) * eps)\n",
    "\n",
    "    return sample, eps  # return ... , gt noise --> model predicts this)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-15T18:11:49.748358Z",
     "start_time": "2024-06-15T18:11:49.726170700Z"
    }
   },
   "id": "b5517589dd3d3a9b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9761e0cf530601e5"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "model = UNet(\n",
    "    input_channels          = TrainingConfig.IMG_SHAPE[0],\n",
    "    output_channels         = TrainingConfig.IMG_SHAPE[0],\n",
    "    base_channels           = ModelConfig.BASE_CH,\n",
    "    base_channels_multiples = ModelConfig.BASE_CH_MULT,\n",
    "    apply_attention         = ModelConfig.APPLY_ATTENTION,\n",
    "    dropout_rate            = ModelConfig.DROPOUT_RATE,\n",
    "    time_multiple           = ModelConfig.TIME_EMB_MULT,\n",
    ")\n",
    "model.to(BaseConfig.DEVICE)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=TrainingConfig.LR)\n",
    "\n",
    "dataloader = get_dataloader(\n",
    "    dataset_name  = BaseConfig.DATASET,\n",
    "    batch_size    = TrainingConfig.BATCH_SIZE,\n",
    "    device        = BaseConfig.DEVICE,\n",
    "    pin_memory    = True,\n",
    "    num_workers   = TrainingConfig.NUM_WORKERS,\n",
    ")\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "sd = SimpleDiffusion(\n",
    "    num_diffusion_timesteps = TrainingConfig.TIMESTEPS,\n",
    "    img_shape               = TrainingConfig.IMG_SHAPE,\n",
    "    device                  = BaseConfig.DEVICE,\n",
    ")\n",
    "\n",
    "scaler = amp.GradScaler()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-15T17:56:06.392228300Z",
     "start_time": "2024-06-15T17:56:06.176402400Z"
    }
   },
   "id": "831f10a16e797370"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "total_epochs = TrainingConfig.NUM_EPOCHS + 1\n",
    "log_dir, checkpoint_dir = setup_log_directory(config=BaseConfig())\n",
    "\n",
    "generate_video = False\n",
    "ext = \".mp4\" if generate_video else \".png\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9525a4dcb7b68e55"
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](.\\resources\\training.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e97c5ad1b1cc505a"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Algorithm 1: Training\n",
    "\n",
    "def train_one_epoch(model, sd, loader, optimizer, scaler, loss_fn, epoch=800, \n",
    "                   base_config=BaseConfig(), training_config=TrainingConfig()):\n",
    "    \n",
    "    loss_record = MeanMetric()\n",
    "    model.train()\n",
    "\n",
    "    with tqdm(total=len(loader), dynamic_ncols=True) as tq:\n",
    "        tq.set_description(f\"Train :: Epoch: {epoch}/{training_config.NUM_EPOCHS}\")\n",
    "    \n",
    "        for x0s, _ in loader:\n",
    "            tq.update(1)\n",
    "            ts = torch.randint(low=1, high=training_config.TIMESTEPS, size=(x0s.shape[0],), device=base_config.DEVICE)\n",
    "            xts, gt_noise = forward_diffusion(sd, x0s, ts)\n",
    "            \n",
    "            with amp.autocast():\n",
    "                pred_noise = model(xts, ts)\n",
    "                loss = loss_fn(gt_noise, pred_noise)\n",
    "                \n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # scaler.unscale_(optimizer)\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            loss_value = loss.detach().item()\n",
    "            loss_record.update(loss_value)\n",
    "\n",
    "            tq.set_postfix_str(s=f\"Loss: {loss_value:.4f}\")\n",
    "\n",
    "        mean_loss = loss_record.compute().item()\n",
    "    \n",
    "        tq.set_postfix_str(s=f\"Epoch Loss: {mean_loss:.4f}\")\n",
    "    \n",
    "    return mean_loss "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-15T18:12:02.344901400Z",
     "start_time": "2024-06-15T18:12:02.327459300Z"
    }
   },
   "id": "a20107da10cc863f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sampling"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21c4ab0a8cbf9b60"
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](.\\resources\\sampling.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bcaa9daf63a69f5c"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Algorithm 2: Sampling\n",
    "    \n",
    "@torch.no_grad()\n",
    "def reverse_diffusion(model, sd, timesteps=1000, img_shape=(3, 64, 64), \n",
    "                      num_images=5, nrow=8, device=\"cpu\", **kwargs):\n",
    "\n",
    "    x = torch.randn((num_images, *img_shape), device=device)\n",
    "    model.eval()\n",
    "\n",
    "    if kwargs.get(\"generate_video\", False):\n",
    "        outs = []\n",
    "\n",
    "    for time_step in tqdm(iterable=reversed(range(1, timesteps)), \n",
    "                          total=timesteps-1, dynamic_ncols=False, \n",
    "                          desc=\"Sampling :: \", position=0):\n",
    "\n",
    "        ts = torch.ones(num_images, dtype=torch.long, device=device) * time_step\n",
    "        z = torch.randn_like(x) if time_step > 1 else torch.zeros_like(x)\n",
    "\n",
    "        predicted_noise = model(x, ts)\n",
    "\n",
    "        beta_t                            = get(sd.beta, ts)\n",
    "        one_by_sqrt_alpha_t               = get(sd.one_by_sqrt_alpha, ts)\n",
    "        sqrt_one_minus_alpha_cumulative_t = get(sd.sqrt_one_minus_alpha_cumulative, ts) \n",
    "\n",
    "        x = (\n",
    "            one_by_sqrt_alpha_t\n",
    "            * (x - (beta_t / sqrt_one_minus_alpha_cumulative_t) * predicted_noise)\n",
    "            + torch.sqrt(beta_t) * z\n",
    "        )\n",
    "\n",
    "        if kwargs.get(\"generate_video\", False):\n",
    "            x_inv = inverse_transform(x).type(torch.uint8)\n",
    "            grid = make_grid(x_inv, nrow=nrow, pad_value=255.0).to(\"cpu\")\n",
    "            ndarr = torch.permute(grid, (1, 2, 0)).numpy()[:, :, ::-1]\n",
    "            outs.append(ndarr)\n",
    "\n",
    "    if kwargs.get(\"generate_video\", False): # Generate and save video of the entire reverse process. \n",
    "        frames2vid(outs, kwargs['save_path'])\n",
    "        display(Image.fromarray(outs[-1][:, :, ::-1])) # Display the image at the final timestep of the reverse process.\n",
    "        return None\n",
    "\n",
    "    else: # Display and save the image at the final timestep of the reverse process. \n",
    "        x = inverse_transform(x).type(torch.uint8)\n",
    "        grid = make_grid(x, nrow=nrow, pad_value=255.0).to(\"cpu\")\n",
    "        pil_image = TF.functional.to_pil_image(grid)\n",
    "        pil_image.save(kwargs['save_path'], format=save_path[-3:].upper())\n",
    "        display(pil_image)\n",
    "        return None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-15T18:12:05.229634400Z",
     "start_time": "2024-06-15T18:12:05.197417200Z"
    }
   },
   "id": "994c47a23b745210"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Everything in action"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7feb94fe9f81c6f5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for epoch in range(1, total_epochs):\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Algorithm 1: Training\n",
    "    train_one_epoch(model, sd, dataloader, optimizer, scaler, loss_fn, epoch=epoch)\n",
    "    #tmp\n",
    "    checkpoint_dict = {\n",
    "            \"opt\": optimizer.state_dict(),\n",
    "            \"scaler\": scaler.state_dict(),\n",
    "            \"model\": model.state_dict()\n",
    "        }\n",
    "    torch.save(checkpoint_dict, os.path.join(checkpoint_dir, \"ckpt.tar\"))\n",
    "    del checkpoint_dict\n",
    "    #\n",
    "    if epoch % 20 == 0:\n",
    "        save_path = os.path.join(log_dir, f\"{epoch}{ext}\")\n",
    "        \n",
    "        # Algorithm 2: Sampling\n",
    "        reverse_diffusion(model, sd, timesteps=TrainingConfig.TIMESTEPS, num_images=32, generate_video=generate_video,\n",
    "            save_path=save_path, img_shape=TrainingConfig.IMG_SHAPE, device=BaseConfig.DEVICE,\n",
    "        )\n",
    "\n",
    "        # clear_output()\n",
    "        checkpoint_dict = {\n",
    "            \"opt\": optimizer.state_dict(),\n",
    "            \"scaler\": scaler.state_dict(),\n",
    "            \"model\": model.state_dict()\n",
    "        }\n",
    "        torch.save(checkpoint_dict, os.path.join(checkpoint_dir, \"ckpt.tar\"))\n",
    "        del checkpoint_dict"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ffbb9daf961a0da"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# !rm -rf /kaggle/working/Logs_Checkpoints"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6212b7960e52be75"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Inference"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c06c9b5186acb21d"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "model = UNet(\n",
    "    input_channels          = TrainingConfig.IMG_SHAPE[0],\n",
    "    output_channels         = TrainingConfig.IMG_SHAPE[0],\n",
    "    base_channels           = ModelConfig.BASE_CH,\n",
    "    base_channels_multiples = ModelConfig.BASE_CH_MULT,\n",
    "    apply_attention         = ModelConfig.APPLY_ATTENTION,\n",
    "    dropout_rate            = ModelConfig.DROPOUT_RATE,\n",
    "    time_multiple           = ModelConfig.TIME_EMB_MULT,\n",
    ")\n",
    "model.load_state_dict(torch.load(os.path.join(checkpoint_dir, \"ckpt.tar\"), map_location='cpu')['model'])\n",
    "\n",
    "model.to(BaseConfig.DEVICE)\n",
    "\n",
    "sd = SimpleDiffusion(\n",
    "    num_diffusion_timesteps = TrainingConfig.TIMESTEPS,\n",
    "    img_shape               = TrainingConfig.IMG_SHAPE,\n",
    "    device                  = BaseConfig.DEVICE,\n",
    ")\n",
    "\n",
    "log_dir = \"inference_results\"\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-15T18:14:00.912837200Z",
     "start_time": "2024-06-15T18:14:00.541578100Z"
    }
   },
   "id": "4da7d8cbce964c35"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "generate_video = True\n",
    "\n",
    "ext = \".mp4\" if generate_video else \".png\"\n",
    "filename = f\"{datetime.now().strftime('%Y%m%d-%H%M%S')}{ext}\"\n",
    "\n",
    "save_path = os.path.join(log_dir, filename)\n",
    "\n",
    "reverse_diffusion(\n",
    "    model,\n",
    "    sd,\n",
    "    num_images=10,\n",
    "    generate_video=generate_video,\n",
    "    save_path=save_path,\n",
    "    timesteps=200,\n",
    "    img_shape=TrainingConfig.IMG_SHAPE,\n",
    "    device=BaseConfig.DEVICE,\n",
    "    nrow=32,\n",
    ")\n",
    "print(save_path)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a3e4d3a20f06eef"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
